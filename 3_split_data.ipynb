{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將所有的 data 分為 80% 訓練以及 20% 的測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from config import Args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 讀取CSV文件\n",
    "data = pd.read_csv(os.path.join(Args.META_DATA_PATH, 'train_metadata.csv'))\n",
    "\n",
    "# 獲取所有不重複的SeriesInstanceUID\n",
    "grouped_data = data.groupby('SeriesInstanceUID')\n",
    "\n",
    "# 初始化空的DataFrame，用來儲存分割後的資料\n",
    "train_data = pd.DataFrame(columns=data.columns)\n",
    "test_data = pd.DataFrame(columns=data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依次處理每個SOPInstanceUID的資料\n",
    "for _, group in grouped_data:\n",
    "    # 決定將這個SOPInstanceUID的資料分到哪個集合（80%放入訓練集，20%放入測試集）\n",
    "    if np.random.rand() < 0.8:\n",
    "        train_data = pd.concat([train_data, group])\n",
    "    else:\n",
    "        test_data = pd.concat([test_data, group])\n",
    "\n",
    "# 將分割後的資料儲存為CSV檔案\n",
    "train_data.to_csv(os.path.join(Args.FL_META_DATA_PATH, 'fl_train_meta_data.csv'), index=False)\n",
    "test_data.to_csv(os.path.join(Args.FL_META_DATA_PATH, 'fl_test_meta_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 將分好的 80% 訓練資料再切成十等分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定要切成幾個等分\n",
    "total_partitions = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(Args.FL_META_DATA_PATH, 'fl_train_meta_data.csv'))\n",
    "group_sort = data.groupby('SeriesInstanceUID')\n",
    "partition = int(len(group_sort) / total_partitions)\n",
    "\n",
    "for i in range(total_partitions):\n",
    "    globals()['site_meta_'+ str(i+1)] = pd.DataFrame(columns=data.columns)\n",
    "\n",
    "\n",
    "group_id = 1\n",
    "for index, (_, group) in enumerate(group_sort):\n",
    "    if group_id > partition:\n",
    "        globals()[f'site_meta_'+str(group_id-1)] = pd.concat([globals()['site_meta_'+str(group_id-1)], group])\n",
    "    else:\n",
    "        globals()[f'site_meta_'+str(group_id)] = pd.concat([globals()['site_meta_'+str(group_id)], group])\n",
    "        if (index+1) % total_partitions == 0:\n",
    "            group_id += 1\n",
    "\n",
    "for i in range(total_partitions):\n",
    "    file_name = f'site_meta_{i+1}.csv'\n",
    "    globals()[f'site_meta_'+str(i+1)].to_csv(os.path.join(Args.FL_SITE_META_DATA_PATH, file_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把 label 和切割好的資料對應出來 ( train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = os.listdir(Args.FL_SITE_META_DATA_PATH)\n",
    "for index, file in enumerate(dirs):\n",
    "    print(file)\n",
    "    # 讀取第一個CSV文件，包含 SeriesInstanceUID 信息\n",
    "    series_data = pd.read_csv(os.path.join(Args.FL_SITE_META_DATA_PATH, file))\n",
    "\n",
    "    # 讀取第二個CSV文件，包含Image對應的SOPInstanceUID信息\n",
    "    image_data = pd.read_csv(os.path.join(Args.TRAIN_CSV_PATH, 'train.csv'))\n",
    "\n",
    "    grouped_series = series_data.groupby('SeriesInstanceUID')\n",
    "\n",
    "    result_df = pd.DataFrame(columns=image_data.columns)\n",
    "\n",
    "    # 將 Image 信息整理到對應的分组中\n",
    "    for index, group in grouped_series:\n",
    "        series_uid = index\n",
    "        sop_instance_ids = list(group['SOPInstanceUID'])\n",
    "        corresponding_images = image_data[image_data['Image'].isin(sop_instance_ids)]\n",
    "        result_df = pd.concat([result_df, corresponding_images])\n",
    "\n",
    "    temp = file.split(\".\")\n",
    "    file_name = temp[0].split(\"_\")\n",
    "\n",
    "    file_path = os.path.join(Args.FL_SITE_CSV_PATH, f\"{file_name[0]}-{file_name[2]}.csv\")\n",
    "    # 將最終的結果保存成 csv\n",
    "    result_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把 label 和切割好的資料對應出來 ( test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_data = pd.read_csv(os.path.join(Args.FL_META_DATA_PATH, 'fl_test_meta_data.csv'))\n",
    "\n",
    "image_data = pd.read_csv(os.path.join(Args.TRAIN_CSV_PATH, 'train.csv'))\n",
    "\n",
    "grouped_series = series_data.groupby('SeriesInstanceUID')\n",
    "result_df = pd.DataFrame(columns=image_data.columns)\n",
    "\n",
    "for index, group in grouped_series:\n",
    "    series_uid = index\n",
    "    sop_instance_ids = list(group['SOPInstanceUID'])\n",
    "    corresponding_images = image_data[image_data['Image'].isin(sop_instance_ids)]\n",
    "    result_df = pd.concat([result_df, corresponding_images])\n",
    "\n",
    "file_path = os.path.join(Args.FL_TEST_CSV_PATH, \"test_data.csv\")\n",
    "result_df.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rsna38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
